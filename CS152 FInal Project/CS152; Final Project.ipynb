{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS152 Final Project\n",
    "# Gesture Controlled PC\n",
    "## Introduction\n",
    "The main purpose of this assignment is to build a robot that (when connected to PC) enables you to control certain movement on your PC with your hands gestures. This feature has recently been introduced in some updated laptops and PC systems but its very costly and requires the user to understand and learn all the gestures if he wants to navigate through his computer efficiently. Moreover, this feature comes with some built in recognized gestures which may or may not have the option to customize and may be incovinient to use for a certain group of people (for example, I being a left handed person find it hard to play a car racing game with control keys in my right hand). This technology is named as \"LEAP\" and it was first introduced by HP into its laptops that use Leap Motion's gesture-based controller embedded directly inside the laptop to recognize the gestures. In this assignment, I am going to document my steps and show how can one transform any PC or laptop into a gesture controlled one in 1/100th the price of the original laptop with the built-in feature. Moreover, this gadget is totally customizeable so people with all demographics can choose the feature they want to control and how they want to control by just changing certain key association or the associated gesture. Although this model will require alot of training and trial and error in order to be as efficient as the in built leap laptops so for this assignment, we are only going to focus on how we can control some specific motions of our PC with gestures. These motions are described as follows:\n",
    "\n",
    "### Objective 1: Controlling the motions of a playing video:\n",
    "For this feature we are going to customize our gadget so that it can control all the features of a video playing chrome applications. These features include\n",
    "1. Pause/Play video\n",
    "2. Increase/decrease volume\n",
    "3. Fast forward/ rewind the video \n",
    "4. Exit the application\n",
    "\n",
    "### Objective 2: Control a Scratch Based game with gestures:\n",
    "For this part of the project I will create a simple scratch game and try to control its motion using my gestures. Scratch basically is a block-based visual programming language and website designed for children learn code. By using this language, people can create projects on the web using a block-like interface. I will use its visual programming feature to showcase myself playing the game and interacting with it in real time. I will convert the inputs from my sensors into keyboard commands and associate them with my game controls. The rationale behind using scratch to build the game is that they are very less complicated. And implementation of this gadget on a complex game (Asphalt 8, Need for speed etc) would require us to build an AI model that would have to be trained according to the maps, noise in the environment, and more complex gaming controls in order to be efficient enough to recognize the gestures and play the game smoothly. This will require alot of time and skills that I will be focusing on for my intended capstone next year. So considering these constraints, I will have to build my own simple game (for now) to serve the purpose of this project.\n",
    "\n",
    "\n",
    "By the end of the paper I will also discuss how similar techniques can be extended and implemented on other PC controls and games. We will also discuss some alternative technologies that can be used to serve a similar purpose and their working such that you'll be able to differentiate between the approaches and their implementation. That being said, let's proceed towards the build up and circuit for the required robot. \n",
    "\n",
    "### Materials Used for building the Robot\n",
    "1. Arduino Uno\n",
    "2. (x2) Ultrasonic Sensors\n",
    "3. (x2) 220 Ohm Resistors\n",
    "4. Connecting wires\n",
    "5. Breadboard\n",
    "6. 9V Battery\n",
    "\n",
    "## Circuit Diagrams\n",
    "\n",
    "![](Circuit_Diagram.png)\n",
    "\n",
    "\n",
    "In the circuit diagram seen above, I have connected two ultrasonic sensors to the Arduino boards. The pin description of this sensor is described below;\n",
    "\n",
    "### Ultrasonic sensor Pin Description\n",
    "VCC: This is the power supply for HC-SR04 Ultrasonic distance sensor which we connect the 5V pin on the Arduino.\n",
    "\n",
    "Trig: This name is the short form for Trigger. This pin is used to trigger the ultrasonic sound pulses. \n",
    "\n",
    "Echo: This pin produces a pulse when the reflected signal is received.\n",
    "\n",
    "Gnd: This pin is the grounded pin and is used as the power outflow for the sensor. It is very useful to complete the circuit\n",
    "### Specifications of the Sensor Used\n",
    "Trigger Input Signal: 10ÂµS TTL pulse\n",
    "\n",
    "Operating Voltage: DC 5V\n",
    "\n",
    "Operating Frequency: 40KHz\n",
    "\n",
    "### Explanation of the Circuit\n",
    "I have connected both the Vcc pins to the 5V power outlet of the Arduino board I have used 2 resistors to control the power inflow for our sensor. Although it is not necessay but this will be helpfull in case we decide to power our gadget through a 9v batter during the circuit testing phase. Both the Gnd pins are grounded. The Trig and echo pins of the sensors occupy pins 7,5 and 6,4 respectively. These will be used to send inputs to the Arduino board which will then be interpretted by python running locally on the machine. The implementation of this circuit diagram can be seen in the figure below which is created using a simulation software\n",
    "\n",
    "\n",
    "### Simulated Circuit Diagram\n",
    "\n",
    "![](Implemented_circuit.png)\n",
    "\n",
    "### Implemented CIrcuit and Building of the Robot\n",
    "![](a.png)\n",
    "![](b.png)\n",
    "![](c.png)\n",
    "\n",
    "## Objective 1: controlling a video with gestures\n",
    "## Working of the Prototype\n",
    "### Recognizeable gestures\n",
    "\n",
    "Gesture 1: When both hands are placed in front of the sensor at a particular distance then the video\n",
    "in VLC player should Play/Pause.\n",
    "\n",
    "\n",
    "Gesture 2: When the right hand is placed in front of the sensor at a particular distance then the\n",
    "video should Fast Forward.\n",
    "\n",
    "\n",
    "Gesture 3: When left hand is placed in front of the sensor at a particular far distance then the video\n",
    "should Rewind one step.\n",
    "\n",
    "\n",
    "Gesture 4: When right hand is in front of the sensor at a near distance and is moved towards the\n",
    "sensor the video should fast forward and if moved away the video should Rewind.\n",
    "\n",
    "\n",
    "Gesture 5: When left hand is in front of the sensor at a near distance and is moved towards the\n",
    "sensor the volume of video should increase and if moved away the volume should Decrease.\n",
    "\n",
    "### Arduino Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arduino Code to get inputs from the sensors and associating different inputs with different recognizeable gestures\n",
    "const int trigger1 = 6; //Trigger pin of 1st Sesnor\n",
    "const int echo1 = 7; //Echo pin of 1st Sesnor\n",
    "const int trigger2 = 4; //Trigger pin of 2nd Sesnor\n",
    "const int echo2 = 5;//Echo pin of 2nd Sesnor\n",
    "\n",
    "long time_taken;\n",
    "int dist,distL,distR;\n",
    "\n",
    "void setup() {\n",
    "Serial.begin(9600); \n",
    "  \n",
    "pinMode(trigger1, OUTPUT); \n",
    "pinMode(echo1, INPUT); \n",
    "pinMode(trigger2, OUTPUT); \n",
    "pinMode(echo2, INPUT); \n",
    "}\n",
    "\n",
    "/*###Function to calculate distance###*/\n",
    "void calculate_distance(int trigger, int echo)\n",
    "{\n",
    "digitalWrite(trigger, LOW);\n",
    "delayMicroseconds(2);\n",
    "digitalWrite(trigger, HIGH);\n",
    "delayMicroseconds(10);\n",
    "digitalWrite(trigger, LOW);\n",
    "\n",
    "time_taken = pulseIn(echo, HIGH);\n",
    "dist= time_taken*0.034/2;\n",
    "if (dist>50)\n",
    "dist = 50;\n",
    "}\n",
    "\n",
    "void loop() {\n",
    "calculate_distance(trigger1,echo1);\n",
    "distL =dist; #get distance of left sensor\n",
    "\n",
    "calculate_distance(trigger2,echo2);\n",
    "distR =dist; #get distance of right sensor\n",
    "\n",
    "\n",
    "#Pause Modes -Hold\n",
    "if ((distL >40 && distR>40) && (distL <50 && distR<50)) //Detect both hands\n",
    "{Serial.println(\"Play/Pause\"); delay (500);}\n",
    "\n",
    "calculate_distance(trigger1,echo1);\n",
    "distL =dist;\n",
    "\n",
    "calculate_distance(trigger2,echo2);\n",
    "distR =dist;\n",
    "\n",
    "#Control Modes\n",
    "#Lock Left - Control Mode\n",
    "if (distL>=13 && distL<=17)\n",
    "{\n",
    "  delay(100); //Hand Hold Time\n",
    "  calculate_distance(trigger1,echo1);\n",
    "  distL =dist;\n",
    "  if (distL>=13 && distL<=17)\n",
    "  {\n",
    "    Serial.println(\"Left Locked\");\n",
    "    while(distL<=40)\n",
    "    {\n",
    "      calculate_distance(trigger1,echo1);\n",
    "      distL =dist;\n",
    "      if (distL<10) //Hand pushed in \n",
    "      {Serial.println (\"Vup\"); delay (300);}\n",
    "      if (distL>20) //Hand pulled out\n",
    "      {Serial.println (\"Vdown\"); delay (300);}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "#Lock Right - Control Mode\n",
    "if (distR>=13 && distR<=17)\n",
    "{\n",
    "  delay(100); //Hand Hold Time\n",
    "  calculate_distance(trigger2,echo2);\n",
    "  distR =dist;\n",
    "  if (distR>=13 && distR<=17)\n",
    "  {\n",
    "    Serial.println(\"Right Locked\");\n",
    "    while(distR<=40)\n",
    "    {\n",
    "      calculate_distance(trigger2,echo2);\n",
    "      distR =dist;\n",
    "      if (distR<10) //Right hand pushed in\n",
    "      {Serial.println (\"Rewind\"); delay (300);}\n",
    "      if (distR>20) //Right hand pulled out\n",
    "      {Serial.println (\"Forward\"); delay (300);}\n",
    "  }\n",
    "}\n",
    "}\n",
    "\n",
    "delay(200);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Arduino Code\n",
    "\n",
    "The above code is the C++ implementation for the working prototype. In this code I first configured all the pins on the arduino board that have a particular sensor pin associated with it. Once the configuration is done, I specify that the trig pins of both the sensors are responsible for the outputs where as both the echo pins are associated with the sensor inputs. Once we have our inputs and outputs configured, I tell the sensor to generate ultrasonic signals for 2 microseconds and then turn on the output interface to receive these signals back. The total time used for these signals to leave the sensor and comeback is then used to calculate the total distance of the object placed infront of the sensors. Once the distance is calculated, it is stored in two variables distL and distR.Here I would like to mention my code calculates this distance for everytime a stimulus is sensed by the sensors, and updates the left and right distance variables. This is why I created the distance measuring function which can be called everytime the sensors sense a stimulus\n",
    "\n",
    "Once we know the distance, we now compare it with predefined values to specify certain metrics that will help us specify certain gestures. As for this identification, instead of using a particular distance value, I used a range between the values to increase the overall time taken to get the inputs. Once I have configured the inputs, I use them in the next section where I use python to associate these inputs with prefered outputs\n",
    "\n",
    "### Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial  #Serial imported for Serial communication\n",
    "import time #Required to use delay functions\n",
    "import pyautogui\n",
    "\n",
    "ArduinoSerial = serial.Serial('COM5',9600) #Create Serial port object called arduinoSerialData\n",
    "time.sleep(2) #wait for 2 seconds for the communication to get established\n",
    "\n",
    "while 1:\n",
    "    incoming = str (ArduinoSerial.readline()) #read the serial data and print it as line\n",
    "    print(incoming)\n",
    "    \n",
    "    if 'Play/Pause' in incoming:\n",
    "        pyautogui.typewrite(['space'], 0.2)\n",
    "        print('Play/Pause')\n",
    "\n",
    "    if 'Rewind' in incoming:\n",
    "        pyautogui.hotkey('ctrl', 'left')\n",
    "        print('Rewind')\n",
    "\n",
    "    if 'Forward' in incoming:\n",
    "        pyautogui.hotkey('ctrl', 'right')\n",
    "        print('Forward')\n",
    "\n",
    "    if 'Vup' in incoming:\n",
    "        pyautogui.hotkey('ctrl', 'down')\n",
    "        print('Volume up')\n",
    "        \n",
    "\n",
    "    if 'Vdown' in incoming:\n",
    "        pyautogui.hotkey('ctrl', 'up')\n",
    "        print('Volume Down')\n",
    "\n",
    "    incoming = \"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of my Python Code\n",
    "The above code is the python implementation for the working prototype that is used to get the end results and perform actions. In simple robotics terms, I can term this part of my implementation to be the end effector that reacts to the stimulus in the environment and generates outputs which in this case are associations with keys on the key board. The stimulus , or in my case hands, are identified by the sensors according to the associated gesture. The actions are performed by creating a serial communication with arduino and then associate certain keyboard shortcuts with the incoming inputs. This is done by using python's pyautogyi module which lets our Python script to control the mouse and keyboard to automate interactions with other applications. Once the actions are performed, it prints out a list of actions performed which can bee seen in the test output below\n",
    "\n",
    "## Test controlling a video with gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"screen-capture (1)_Trim.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"screen-capture (1)_Trim.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Generated Outputs During Prototype Testing\n",
    "\n",
    "![](d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 2: Controlling a Scratch based game with Gestures\n",
    "### Arduino Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arduino Code to get inputs from the sensors and associating different inputs with different recognizeable gestures\n",
    "const int trigger1 = 6; //Trigger pin of 1st Sesnor\n",
    "const int echo1 = 7; //Echo pin of 1st Sesnor\n",
    "const int trigger2 = 4; //Trigger pin of 2nd Sesnor\n",
    "const int echo2 = 5;//Echo pin of 2nd Sesnor\n",
    "\n",
    "long time_taken;\n",
    "int dist,distL,distR;\n",
    "\n",
    "void setup() {\n",
    "Serial.begin(9600); \n",
    "  \n",
    "pinMode(trigger1, OUTPUT); \n",
    "pinMode(echo1, INPUT); \n",
    "pinMode(trigger2, OUTPUT); \n",
    "pinMode(echo2, INPUT); \n",
    "}\n",
    "\n",
    "/*###Function to calculate distance###*/\n",
    "void calculate_distance(int trigger, int echo)\n",
    "{\n",
    "digitalWrite(trigger, LOW);\n",
    "delayMicroseconds(2);\n",
    "digitalWrite(trigger, HIGH);\n",
    "delayMicroseconds(10);\n",
    "digitalWrite(trigger, LOW);\n",
    "\n",
    "time_taken = pulseIn(echo, HIGH);\n",
    "dist= time_taken*0.034/2;\n",
    "if (dist>50)\n",
    "dist = 50;\n",
    "}\n",
    "\n",
    "void loop() {\n",
    "calculate_distance(trigger1,echo1);\n",
    "distL =dist; #get distance of left sensor\n",
    "\n",
    "calculate_distance(trigger2,echo2);\n",
    "distR =dist; #get distance of right sensor\n",
    "\n",
    "#Control Modes\n",
    "#Lock Left - Control Mode\n",
    "if (distL>=13 && distL<=17)\n",
    "{\n",
    "  delay(100); //Hand Hold Time\n",
    "  calculate_distance(trigger1,echo1);\n",
    "  distL =dist;\n",
    "  if (distL>=13 && distL<=17)\n",
    "  {\n",
    "    Serial.println(\"Left Locked\");\n",
    "    while(distL<=40)\n",
    "    {\n",
    "      calculate_distance(trigger1,echo1);\n",
    "      distL =dist;\n",
    "      if (distL<10) //Hand pushed in \n",
    "      {Serial.println (\"MoveLeft\"); delay (300);}\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "#Lock Right - Control Mode\n",
    "if (distR>=13 && distR<=17)\n",
    "{\n",
    "  delay(100); //Hand Hold Time\n",
    "  calculate_distance(trigger2,echo2);\n",
    "  distR =dist;\n",
    "  if (distR>=13 && distR<=17)\n",
    "  {\n",
    "    Serial.println(\"Right Locked\");\n",
    "    while(distR<=40)\n",
    "    {\n",
    "      calculate_distance(trigger2,echo2);\n",
    "      distR =dist;\n",
    "      if (distR<10) //Right hand pushed in\n",
    "      {Serial.println (\"MoveRight\"); delay (300);}\n",
    "  }\n",
    "}\n",
    "}\n",
    "\n",
    "delay(200);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial  #Serial imported for Serial communication\n",
    "import time #Required to use delay functions\n",
    "import pyautogui\n",
    "\n",
    "ArduinoSerial = serial.Serial('COM5',9600) #Create Serial port object called arduinoSerialData\n",
    "time.sleep(2) #wait for 2 seconds for the communication to get established\n",
    "\n",
    "while 1:\n",
    "    incoming = str (ArduinoSerial.readline()) #read the serial data and print it as line\n",
    "    print(incoming)\n",
    "\n",
    "    if 'MoveLeft' in incoming:\n",
    "        pyautogui.hotkey('ctrl','left')\n",
    "        print('Moving Left')\n",
    "\n",
    "    if 'MoveRight' in incoming:\n",
    "        pyautogui.hotkey('ctrl','right')\n",
    "        print('MoveRight')\n",
    "\n",
    "    incoming = \"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch Game Discription and Associated Gestures\n",
    "\n",
    "Game link: https://scratch.mit.edu/projects/466143079\n",
    "\n",
    "Here I created this simple scratch game where a bunch of aliens appear on the top of the screen and you have to bounce the ball, in my case planet, over a flat surface to kill as many of them possible in order to save the galaxy. The code for the game can be seen by clicking at the link above. Now I am goint to test run this game using my updated arduino code and associated python code. Here I just used two gestures\n",
    "\n",
    "Gesture 1: When the left hand is placed in front of the left sensor at a particular distance and then moved closer to it then the flat surface should head to left\n",
    "Gesture 2: When the right hand is placed in front of the left sensor at a particular distance then the flat surface should head to Right\n",
    "\n",
    "### Test Controlling the Scratch Based game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"screen-capture (2).mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"screen-capture (2).mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses and Conclusion\n",
    "\n",
    "Throughout this paper, I documented my steps while creating and prototyping a sensor based gesture controlled computer. I tested the efficiency of my gadget, by putting it through 2 test, I first tried to control a video and all motions within that video, secondly, I tested a very similar approach on a scratch based game. Both the results show that my prototype is efficient enough and lets the user to control several different application with a bunch of already recognized gestures. That being said, we can extend upon this idea and create some more gestures that will help us control mouse opening and closing of browsers, scrolling etc. These text experiments show the potential of research that can be done into making such a product and how it can be made more efficient over time. Although there are a bunch of downfalls to this approch, one of which is that It is highly senitive so using the device on several different PC's would be very hard. Moreover,  it can only be used while the PC is positioned at a particular place and may require further calibration of sensors if moved to a new place. Its efficiency can also be increased by using high power sensors etc. but the overall cost associated with such a gadget is atmost $100 (mine cost 30) which is still 1/10th the cost of PC that has build in Leap technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LO's Used\n",
    "\n",
    "#Robotics: Here I used this LO to create and implement a sensor based gadget that would enable its users to control the PC by simple gestures of hands. I also explained the build up by including circuit diagrams and simulated those circuits to test the accurate configuration and calibration of the sensors. Moreover, I implemented the circuit diagram and created a prototype of the said robot. I tested and explained the robot in two different applications with 2 different output results and also explained the potential of this gadget for more research. Additionally, I critiqued the prototype and suggested some ways this can be made more efficient\n",
    "\n",
    "\n",
    "## HC's Used\n",
    "\n",
    "#Simulation: Here I used this HC to create simulations for the working of my prototype. I used an online software known as tinkercad to devise the circuit and coded my arduino according to the accurate calubration of sensors with the least response time which made my prototype more efficient. I then tested this efficieny in a series of test and documented all my steps through it\n",
    "\n",
    "Algorithms: Here I used this HC to accurately write a Python and C++ code for my prototype. This algorithm works very well and the working of my prototype is very efficient as demonsrated in the videos.\n",
    "\n",
    "\n",
    "## References\n",
    "Kaoru Y, Lie J, Cheng Z (2014) System for controlling personal computers by hand gestures\n",
    "using a wireless sensor device. IEEE\n",
    "\n",
    "\n",
    "Nguyen MQ, Li C (2010) Radar and ultrasound hybrid system for human-computer interaction.\n",
    "IEEE\n",
    "\n",
    "\n",
    "Sziladi G, Ujbanyi T, Katona J, Kovari A (2017) The analysis of hand gesture-based cursor\n",
    "position control during solving an IT-related task. 2017 8th IEEE International Conference on Cognitive Infocommunications (CogInfoCom), Debrecen, pp. 000413â000418\n",
    "\n",
    "\n",
    "Rajkanna U, Mathankumar M, Gunsekaran K (2014) Hand gesture-based mobile robot control using PIC microcontroller. 2014 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2014], Nagercoil, pp. 1687â1691\n",
    "\n",
    "Anju Latha N, Rama Murthy B, Bharat Kumar K (2016) distance sensing with ultrasonic sensor\n",
    "and Arduino. Department of Instrumentation, Sri Krishnadevaraya University, Anantapur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "Video Link: https://www.youtube.com/watch?v=XCppmIdPLVo&feature=youtu.be\n",
    "\n",
    "\n",
    "Github Repository Link: https://github.com/Usamapuri/Gesture-Controlled-Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
